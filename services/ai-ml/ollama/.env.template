# Ollama LLM Service Configuration

# Port Configuration
OLLAMA_PORT=11434

# GPU Support (uncomment for CUDA)
# OLLAMA_GPU=1
# Add to docker-compose.yml:
#   deploy:
#     resources:
#       reservations:
#         devices:
#           - driver: nvidia
#             count: 1
#             capabilities: [gpu]
